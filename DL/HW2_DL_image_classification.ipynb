{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr9vAeEQlRVG"
   },
   "source": [
    "# Домашнее задание 2. Классификация изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxX49gLclRVJ"
   },
   "source": [
    "В этом задании потребуется обучить классификатор изображений. Будем работать с датасетом, название которого раскрывать не будем. Можете посмотреть самостоятельно на картинки, которые в есть датасете. В нём 200 классов и около 5 тысяч картинок на каждый класс. Классы пронумерованы, как нетрудно догадаться, от 0 до 199. Скачать датасет можно вот [тут](https://yadi.sk/d/BNR41Vu3y0c7qA).\n",
    "\n",
    "Структура датасета простая -- есть директории train/ и val/, в которых лежат обучающие и валидационные данные. В train/ и val/ лежат директориии, соответствующие классам изображений, в которых лежат, собственно, сами изображения.\n",
    " \n",
    "__Задание__. Необходимо выполнить любое из двух заданий\n",
    "\n",
    "1) Добейтесь accuracy **на валидации не менее 0.44**. В этом задании **запрещено** пользоваться предобученными моделями и ресайзом картинок. \n",
    "\n",
    "2) Добейтесь accuracy **на валидации не менее 0.84**. В этом задании делать ресайз и использовать претрейн можно. \n",
    "\n",
    "Напишите краткий отчёт о проделанных экспериментах. Что сработало и что не сработало? Почему вы решили, сделать так, а не иначе? Обязательно указывайте ссылки на чужой код, если вы его используете. Обязательно ссылайтесь на статьи / блогпосты / вопросы на stackoverflow / видосы от ютуберов-машинлернеров / курсы / подсказки от Дяди Васи и прочие дополнительные материалы, если вы их используете. \n",
    "\n",
    "Ваш код обязательно должен проходить все `assert`'ы ниже.\n",
    "\n",
    "Необходимо написать функции `train_one_epoch`, `train` и `predict` по шаблонам ниже (во многом повторяют примеры с семинаров).Обратите особое внимание на функцию `predict`: она должна возвращать список лоссов по всем объектам даталоадера, список предсказанных классов для каждого объекта из даталоалера и список настоящих классов для каждого объекта в даталоадере (и именно в таком порядке).\n",
    "\n",
    "__Использовать внешние данные для обучения строго запрещено в обоих заданиях. Также запрещено обучаться на валидационной выборке__.\n",
    "\n",
    "\n",
    "__Критерии оценки__: Оценка вычисляется по простой формуле: `min(10, 10 * Ваша accuracy / 0.44)` для первого задания и `min(10, 10 * (Ваша accuracy - 0.5) / 0.34)` для второго. Оценка округляется до десятых по арифметическим правилам. Если вы выполнили оба задания, то берется максимум из двух оценок.\n",
    "\n",
    "__Бонус__. Вы получаете 5 бонусных баллов если справляетесь с обоими заданиями на 10 баллов (итого 15 баллов). В противном случае выставляется максимальная из двух оценок и ваш бонус равен нулю.\n",
    "\n",
    "__Советы и указания__:\n",
    " - Наверняка вам потребуется много гуглить о классификации и о том, как заставить её работать. Это нормально, все гуглят. Но не забывайте, что нужно быть готовым за скатанный код отвечать :)\n",
    " - Используйте аугментации. Для этого пользуйтесь модулем `torchvision.transforms` или библиотекой [albumentations](https://github.com/albumentations-team/albumentations)\n",
    " - Можно обучать с нуля или файнтюнить (в зависимости от задания) модели из `torchvision`.\n",
    " - Рекомендуем написать вам сначала класс-датасет (или воспользоваться классом `ImageFolder`), который возвращает картинки и соответствующие им классы, а затем функции для трейна по шаблонам ниже. Однако делать это мы не заставляем. Если вам так неудобно, то можете писать код в удобном стиле. Однако учтите, что чрезмерное изменение нижеперечисленных шаблонов увеличит количество вопросов к вашему коду и повысит вероятность вызова на защиту :)\n",
    " - Валидируйте. Трекайте ошибки как можно раньше, чтобы не тратить время впустую.\n",
    " - Чтобы быстро отладить код, пробуйте обучаться на маленькой части датасета (скажем, 5-10 картинок просто чтобы убедиться что код запускается). Когда вы поняли, что смогли всё отдебажить, переходите обучению по всему датасету\n",
    " - На каждый запуск делайте ровно одно изменение в модели/аугментации/оптимайзере, чтобы понять, что и как влияет на результат.\n",
    " - Фиксируйте random seed.\n",
    " - Начинайте с простых моделей и постепенно переходите к сложным. Обучение лёгких моделей экономит много времени.\n",
    " - Ставьте расписание на learning rate. Уменьшайте его, когда лосс на валидации перестаёт убывать.\n",
    " - Советуем использовать GPU. Если у вас его нет, используйте google colab. Если вам неудобно его использовать на постоянной основе, напишите и отладьте весь код локально на CPU, а затем запустите уже написанный ноутбук в колабе. Авторское решение задания достигает требуемой точности в колабе за 15 минут обучения.\n",
    " \n",
    "Good luck & have fun! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:13:41.276811Z",
     "iopub.status.busy": "2021-11-23T21:13:41.276038Z",
     "iopub.status.idle": "2021-11-23T21:13:53.156326Z",
     "shell.execute_reply": "2021-11-23T21:13:53.155536Z",
     "shell.execute_reply.started": "2021-11-23T21:13:41.276719Z"
    },
    "id": "X6LQ50LkLMZi",
    "outputId": "faf1ca57-941f-4af6-bf19-185184ac1740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.5)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 909 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.4.3)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.0)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.8.0)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.23)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (5.4.1)\n",
      "Requirement already satisfied: yaspin>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.8.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.5.0)\n",
      "Installing collected packages: wandb\n",
      "  Attempting uninstall: wandb\n",
      "    Found existing installation: wandb 0.12.5\n",
      "    Uninstalling wandb-0.12.5:\n",
      "      Successfully uninstalled wandb-0.12.5\n",
      "Successfully installed wandb-0.12.7\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:13:53.158626Z",
     "iopub.status.busy": "2021-11-23T21:13:53.158375Z",
     "iopub.status.idle": "2021-11-23T21:13:56.130329Z",
     "shell.execute_reply": "2021-11-23T21:13:56.129485Z",
     "shell.execute_reply.started": "2021-11-23T21:13:53.158599Z"
    },
    "id": "LKcSNj4tlRVK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RytEDW0ylRVN"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T11:34:09.624105Z",
     "iopub.status.busy": "2021-11-23T11:34:09.623849Z",
     "iopub.status.idle": "2021-11-23T11:34:09.62937Z",
     "shell.execute_reply": "2021-11-23T11:34:09.627772Z",
     "shell.execute_reply.started": "2021-11-23T11:34:09.624068Z"
    },
    "id": "mr6AInU4TZwy",
    "outputId": "c29c0c13-019b-496f-d8ac-7e1adbaf0264"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:13:56.131909Z",
     "iopub.status.busy": "2021-11-23T21:13:56.131670Z",
     "iopub.status.idle": "2021-11-23T21:14:11.580637Z",
     "shell.execute_reply": "2021-11-23T21:14:11.579898Z",
     "shell.execute_reply.started": "2021-11-23T21:13:56.131877Z"
    },
    "id": "GKPnNKPSEigC",
    "outputId": "038ce7cc-adec-439a-87e2-e9360038fafb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m No API key specified.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:15:52.878626Z",
     "iopub.status.busy": "2021-11-23T21:15:52.878123Z",
     "iopub.status.idle": "2021-11-23T21:15:52.886226Z",
     "shell.execute_reply": "2021-11-23T21:15:52.885573Z",
     "shell.execute_reply.started": "2021-11-23T21:15:52.878588Z"
    },
    "id": "sC0rSI65qN2O"
   },
   "outputs": [],
   "source": [
    "# Tnx to https://medium.com/analytics-vidhya/creating-a-custom-dataset-and-dataloader-in-pytorch-76f210a1df5d  for deeper understanding\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, transform):\n",
    "        self.imgs_path = os.path.expanduser(data_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        file_list = glob.glob(self.imgs_path + \"*\")\n",
    "\n",
    "        self.data = []\n",
    "        for class_path in file_list:\n",
    "            class_name = int(class_path.split(\"/\")[-1][-3:])\n",
    "            for img_path in glob.glob(class_path + \"/*.jpg\"):\n",
    "                self.data.append([img_path, class_name])\n",
    "        self.img_dim = (416, 416)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_name = self.data[idx]\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "          \n",
    "        return img, class_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:15:53.302901Z",
     "iopub.status.busy": "2021-11-23T21:15:53.302432Z",
     "iopub.status.idle": "2021-11-23T21:15:53.307957Z",
     "shell.execute_reply": "2021-11-23T21:15:53.306970Z",
     "shell.execute_reply.started": "2021-11-23T21:15:53.302866Z"
    },
    "id": "w3EeLdCBXivm"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:15:57.981874Z",
     "iopub.status.busy": "2021-11-23T21:15:57.981406Z",
     "iopub.status.idle": "2021-11-23T21:16:42.507602Z",
     "shell.execute_reply": "2021-11-23T21:16:42.506814Z",
     "shell.execute_reply.started": "2021-11-23T21:15:57.981836Z"
    },
    "id": "QEdDQtHdlRVO",
    "outputId": "ca01115a-148f-46b1-faaf-c689b2164ea6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py:1231: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  \"Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\"\n"
     ]
    }
   ],
   "source": [
    "#train_transform = None\n",
    "#val_transform = None\n",
    "\n",
    "'''\n",
    "# Normalisaion parameters for efficientnet version1:\n",
    "mean = torch.tensor([0.05438065, 0.05291743, 0.07920227])\n",
    "std = torch.tensor([0.39414383, 0.33547948, 0.38544176])\n",
    "\n",
    "size = 300\n",
    "\n",
    "\n",
    "# Normalisaion parameters for resnet:\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "size = 224\n",
    "'''\n",
    "\n",
    "train_transform_basic = transforms.Compose(\n",
    "    [transforms.RandomCrop(32),\n",
    "     transforms.Resize((64,64)),\n",
    "     transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "'''\n",
    "# If we want to use pretrained models we have to normalize data as following\n",
    "# source: https://towardsdatascience.com/pytorch-ignite-classifying-tiny-imagenet-with-efficientnet-e5b1768e5e8f)\n",
    "\n",
    "train_transform_for_pretrained = transforms.Compose([\n",
    "                transforms.Resize((size + 4, size + 4)),\n",
    "                transforms.CenterCrop(size), # Center crop image\n",
    "                #transforms.RandomRotation(40),\n",
    "                #transforms.RandomAffine(\n",
    "                #    degrees=10,\n",
    "                #    translate=(0.01, 0.12),\n",
    "                #    shear=(0.01, 0.03)),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),  # Converting cropped images to tensors\n",
    "                transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "val_transform_for_pretrained = transforms.Compose([\n",
    "                transforms.Resize((size, size)),\n",
    "                transforms.ToTensor(),  # Converting cropped images to tensors\n",
    "                transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "\n",
    "\"./dataset/dataset/train/\" for jupiter\n",
    "/content/gdrive/MyDrive/ColabNotebooks/dataset/dataset/train/ for colab\n",
    "\n",
    "'''\n",
    "\n",
    "train_dataset = MyDataset(data_dir = \"/kaggle/input/hw2-dataset/dataset/dataset/train/\", transform = train_transform_basic)\n",
    "val_dataset = MyDataset(data_dir = \"/kaggle/input/hw2-dataset/dataset/dataset/val/\", transform = train_transform_basic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:16:42.514545Z",
     "iopub.status.busy": "2021-11-23T21:16:42.512536Z",
     "iopub.status.idle": "2021-11-23T21:16:42.521795Z",
     "shell.execute_reply": "2021-11-23T21:16:42.521150Z",
     "shell.execute_reply.started": "2021-11-23T21:16:42.514490Z"
    },
    "id": "H3VHPM6Qs37E"
   },
   "outputs": [],
   "source": [
    "def data_loaders(bs):\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=4)\n",
    "    test_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=bs, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:16:42.529963Z",
     "iopub.status.busy": "2021-11-23T21:16:42.527155Z",
     "iopub.status.idle": "2021-11-23T21:16:42.644077Z",
     "shell.execute_reply": "2021-11-23T21:16:42.643413Z",
     "shell.execute_reply.started": "2021-11-23T21:16:42.529921Z"
    },
    "id": "mrg4Yj0VlRVP",
    "outputId": "68ac221b-9210-4394-ebd7-87cdb4e3a10b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "# Just very simple sanity checks\n",
    "assert isinstance(train_dataset[0], tuple)\n",
    "assert len(train_dataset[0]) == 2\n",
    "assert isinstance(train_dataset[1][1], int)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RlSlmyjlRVP"
   },
   "source": [
    "### Вспомогательные функции, реализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:17:33.180269Z",
     "iopub.status.busy": "2021-11-23T21:17:33.179785Z",
     "iopub.status.idle": "2021-11-23T21:17:33.194061Z",
     "shell.execute_reply": "2021-11-23T21:17:33.193137Z",
     "shell.execute_reply.started": "2021-11-23T21:17:33.180232Z"
    },
    "id": "pnHu6w1FnmgN",
    "outputId": "af7c6bd4-958c-4f85-8676-8c45024db7d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass EarlyStopping():\\n    \"\"\"\\n    Early stopping to stop the training when the loss does not improve after\\n    certain epochs.\\n    \"\"\"\\n    def __init__(self, patience=10, min_delta=0):\\n        \"\"\"\\n        :param patience: how many epochs to wait before stopping when loss is\\n               not improving\\n        :param min_delta: minimum difference between new loss and old loss for\\n               new loss to be considered as an improvement\\n        \"\"\"\\n        self.patience = patience\\n        self.min_delta = min_delta\\n        self.counter = 0\\n        self.best_loss = None\\n        self.early_stop = False\\n\\n    def __call__(self, val_loss):\\n        if self.best_loss == None:\\n            self.best_loss = val_loss\\n        elif self.best_loss - val_loss > self.min_delta:\\n            self.best_loss = val_loss\\n\\n            # Reset counter if validation loss improves\\n            self.counter = 0\\n        elif self.best_loss - val_loss < self.min_delta:\\n            self.counter += 1\\n            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\\n\\n            if self.counter >= self.patience:\\n                print(\\'INFO: Early stopping\\')\\n                self.early_stop = True\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LR scheduler class - to adjust lr if loss doesn't change for a few epochs\n",
    "# Early stopping class - to stop training if loss doesn't change for a long time\n",
    "\n",
    "# Source: https://debuggercafe.com/using-learning-rate-scheduler-and-early-stopping-with-pytorch/ \n",
    "\n",
    "class LRScheduler():\n",
    "    \"\"\"\n",
    "    Learning rate scheduler. If the validation loss does not decrease for the \n",
    "    given number of `patience` epochs, then the learning rate will decrease by\n",
    "    by given `factor`.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, optimizer, patience=5, min_lr=1e-6, factor=0.5\n",
    "    ):\n",
    "        \"\"\"\n",
    "        new_lr = old_lr * factor\n",
    "\n",
    "        :param optimizer: the optimizer we are using\n",
    "        :param patience: how many epochs to wait before updating the lr\n",
    "        :param min_lr: least lr value to reduce to while updating\n",
    "        :param factor: factor by which the lr should be updated\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.factor = factor\n",
    "\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau( \n",
    "                self.optimizer,\n",
    "                mode='min',\n",
    "                patience=self.patience,\n",
    "                factor=self.factor,\n",
    "                min_lr=self.min_lr,\n",
    "                verbose=True\n",
    "            )\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        self.lr_scheduler.step(val_loss)\n",
    "\n",
    "# I decided not to use early stopping as I was training my models only for 10 epochs \n",
    "'''\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "\n",
    "            # Reset counter if validation loss improves\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:23:28.728289Z",
     "iopub.status.busy": "2021-11-23T21:23:28.728032Z",
     "iopub.status.idle": "2021-11-23T21:23:28.745252Z",
     "shell.execute_reply": "2021-11-23T21:23:28.744584Z",
     "shell.execute_reply.started": "2021-11-23T21:23:28.728261Z"
    },
    "id": "P89WqaEqFw48"
   },
   "outputs": [],
   "source": [
    "# Functions from seminar #6\n",
    "\n",
    "def train_one_epoch(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    return_losses=False,\n",
    "    device=\"cuda:0\",\n",
    "):\n",
    "    seed_everything(13)\n",
    "    model = model.to(device).train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    all_losses = []\n",
    "    total_predictions = np.array([])#.reshape((0, ))\n",
    "    total_labels = np.array([])#.reshape((0, ))\n",
    "    with tqdm(total=len(train_dataloader), file=sys.stdout) as prbar:\n",
    "        for images, labels in train_dataloader:\n",
    "          \n",
    "            # Move Batch to GPU\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predicted = model(images)\n",
    "            loss = criterion(predicted, labels)\n",
    "\n",
    "            # Update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update descirption for tqdm\n",
    "            accuracy = (predicted.argmax(1) == labels).float().mean()\n",
    "            wandb.log({\"batch loss\": loss.item()})\n",
    "            wandb.log({\"batch accuracy\": accuracy.item() * 100})\n",
    "            prbar.set_description(\n",
    "                f\"Loss: {round(loss.item(), 4)} \"\n",
    "                f\"Accuracy: {round(accuracy.item() * 100, 4)}\"\n",
    "            )\n",
    "            prbar.update(1)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_predictions = np.append(total_predictions, predicted.argmax(1).cpu().detach().numpy())\n",
    "            total_labels = np.append(total_labels, labels.cpu().detach().numpy())\n",
    "            num_batches += 1\n",
    "            all_losses.append(loss.detach().item())\n",
    "\n",
    "    metrics = {\"loss\": total_loss / num_batches}\n",
    "    metrics.update({\"accuracy\": (total_predictions == total_labels).mean()})\n",
    "\n",
    "    if return_losses:\n",
    "        return metrics, all_losses\n",
    "    else:\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def predict(model, val_dataloder, criterion, device=\"cuda:0\"):\n",
    "\n",
    "    seed_everything(13)\n",
    "    model = model.eval()\n",
    "    total_loss = []\n",
    "    num_batches = 0\n",
    "    total_predictions = np.array([])\n",
    "    total_labels = np.array([])\n",
    "\n",
    "    # New tqdm run\n",
    "    with tqdm(total=len(val_dataloder), file=sys.stdout) as prbar:\n",
    "        for images, labels in val_dataloder:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predicted = model(images)\n",
    "            loss = criterion(predicted, labels)\n",
    "            accuracy = (predicted.argmax(1) == labels).float().mean()\n",
    "            prbar.set_description(\n",
    "                f\"Loss: {round(loss.item(), 4)} \"\n",
    "                f\"Accuracy: {round(accuracy.item() * 100, 4)}\"\n",
    "            )\n",
    "            prbar.update(1)\n",
    "            total_loss.append(loss.item())\n",
    "            total_predictions = np.append(total_predictions, predicted.argmax(1).cpu().detach().numpy())\n",
    "            total_labels = np.append(total_labels, labels.cpu().detach().numpy())\n",
    "            num_batches += 1\n",
    "\n",
    "    metrics = {\"loss_avg\": sum(total_loss) / num_batches}\n",
    "    metrics.update({\"accuracy_avg\": (total_predictions == total_labels).mean()})\n",
    "\n",
    "    all_losses = total_loss\n",
    "    predicted_classes = total_predictions\n",
    "    true_classes = total_labels\n",
    "    \n",
    "    return metrics, all_losses, predicted_classes, true_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:42:04.833799Z",
     "iopub.status.busy": "2021-11-23T21:42:04.833431Z",
     "iopub.status.idle": "2021-11-23T21:42:04.847009Z",
     "shell.execute_reply": "2021-11-23T21:42:04.845796Z",
     "shell.execute_reply.started": "2021-11-23T21:42:04.833760Z"
    },
    "id": "K7r4P5YWLYn6"
   },
   "outputs": [],
   "source": [
    "# commented cells are for normal evaluation, uncommented are for wandb run\n",
    "\n",
    "def train(config=None):\n",
    "\n",
    "    # Initialize a new wandb run - after finding the best parameters -> mode = \"disabled\" to train without wandb\n",
    "    with wandb.init(config=config, mode = \"disabled\"):\n",
    "        config = wandb.config\n",
    "\n",
    "        device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        model = get_model(device, config.dropout_value, config.dropout_2, config.dropout_4,\n",
    "                        config.dropout_6, config.batchnorm_3, config.batchnorm_5,\n",
    "                        config.n_features1, config.n_features2)\n",
    "        '''\n",
    "        model = get_model(device = device, dropout_value = 0.3, dropout_2 = False, dropout_4 = False,\n",
    "                          dropout_6 = False, batchnorm_3 = True, batchnorm_5 = True, n_features1 = 256, n_features2 = 128)\n",
    "        \n",
    "        '''\n",
    "      # Initializing out parameters from config, constructing model\n",
    "        \n",
    "        train_dataloader, val_dataloader = data_loaders(bs = config.batch_size)\n",
    "\n",
    "        '''\n",
    "        train_dataloader, val_dataloader = data_loaders(bs = 32)\n",
    "        optimizer = build_optimizer(model, \"adam\", 0.0001)\n",
    "        lr_scheduler = LRScheduler(optimizer)\n",
    "        '''\n",
    "        #early_stopping = EarlyStopping()\n",
    "        \n",
    "        seed_everything(13)\n",
    "        all_train_losses = []\n",
    "        epoch_train_losses = []\n",
    "        epoch_eval_losses = []\n",
    "        printing = False # in order to avoid a lot of printing on a screen after setting up parameters\n",
    "        \n",
    "\n",
    "        for epoch in range(config.epochs):\n",
    "          if printing:\n",
    "            print(f\"Train Epoch: {epoch}\")\n",
    "          train_metrics, one_epoch_train_losses = train_one_epoch(\n",
    "              model=model,\n",
    "              train_dataloader=train_dataloader,\n",
    "              optimizer=optimizer,\n",
    "              return_losses=True,\n",
    "              criterion=criterion,\n",
    "              device=device\n",
    "            )\n",
    "          # Save Train losses\n",
    "          all_train_losses.extend(one_epoch_train_losses)\n",
    "          epoch_train_losses.append(train_metrics[\"loss\"])\n",
    "\n",
    "          # Eval step\n",
    "          if printing:\n",
    "                print(f\"Validation Epoch: {epoch}\")\n",
    "\n",
    "          with torch.no_grad():\n",
    "             validation_metrics = predict(\n",
    "                  model=model,\n",
    "                  val_dataloder=val_dataloader,\n",
    "                  criterion=criterion)\n",
    "            \n",
    "          # Unproductivity check \n",
    "          lr_scheduler(val_epoch_loss)\n",
    "            \n",
    "          #early_stopping(val_epoch_loss)\n",
    "          #if early_stopping.early_stop:\n",
    "          #  break\n",
    "        \n",
    "          # Add metrics to wandb\n",
    "          wandb.log({\"mean val loss\": validation_metrics[0][\"loss_avg\"],\n",
    "                      \"mean val accuracy\": validation_metrics[0][\"accuracy_avg\"],\n",
    "                      \"mean train accuracy\": train_metrics[\"accuracy\"],\n",
    "                      \"mean train loss\": train_metrics[\"loss\"],\n",
    "                      \"epoch\" : epoch})\n",
    "\n",
    "           # Save eval losses\n",
    "          epoch_eval_losses.append(validation_metrics[0][\"loss_avg\"])\n",
    "          if printing:\n",
    "                print(f\"Validation loss (mean per batch): {validation_metrics[0]['loss_avg']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:21:11.261386Z",
     "iopub.status.busy": "2021-11-23T21:21:11.260799Z",
     "iopub.status.idle": "2021-11-23T21:21:11.266446Z",
     "shell.execute_reply": "2021-11-23T21:21:11.265668Z",
     "shell.execute_reply.started": "2021-11-23T21:21:11.261350Z"
    },
    "id": "gmN4RdRg5fdN"
   },
   "outputs": [],
   "source": [
    "def build_optimizer(network, optimizer, learning_rate):\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = optim.SGD(network.parameters(),\n",
    "                              lr=learning_rate, momentum=0.9)\n",
    "    elif optimizer == \"adam\":\n",
    "        optimizer = optim.Adam(network.parameters(),\n",
    "                               lr=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7rNpb395sce"
   },
   "source": [
    "### Mодели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:21:16.993761Z",
     "iopub.status.busy": "2021-11-23T21:21:16.993417Z",
     "iopub.status.idle": "2021-11-23T21:21:17.014960Z",
     "shell.execute_reply": "2021-11-23T21:21:17.014218Z",
     "shell.execute_reply.started": "2021-11-23T21:21:16.993725Z"
    },
    "id": "UTkMptjZ-8yc"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, dropout_value, dropout_2, dropout_4, dropout_6, batchnorm_3,\n",
    "                 batchnorm_5, n_features1, n_features2):\n",
    "\n",
    "        super().__init__()\n",
    "        seed_everything(13)\n",
    "\n",
    "        self.batch_norm1 = torch.nn.BatchNorm2d(3)\n",
    "        # First conv layer\n",
    "        self.conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU())\n",
    "\n",
    "        # Second conv layer\n",
    "        self.conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        if dropout_2:\n",
    "            self.dropout2 = torch.nn.Dropout(p=0.2)\n",
    "        else:\n",
    "          self.dropout2 = torch.nn.Dropout(p=0)\n",
    "\n",
    "        # Third conv layer\n",
    "        if batchnorm_3:\n",
    "            self.conv3 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "                torch.nn.BatchNorm2d(32),\n",
    "                torch.nn.ReLU())\n",
    "        \n",
    "        else:\n",
    "            self.conv3 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "                torch.nn.ReLU())\n",
    "        \n",
    "        # Fourth conv layer\n",
    "        self.conv4 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "          \n",
    "        if dropout_4:\n",
    "            self.dropout4 = torch.nn.Dropout(p=max(0.2, dropout_value - 0.2))\n",
    "        else:\n",
    "          self.dropout4 = torch.nn.Dropout(p=0)\n",
    "\n",
    "        # Fifth conv layer\n",
    "        if batchnorm_5:\n",
    "            self.conv5 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "                torch.nn.BatchNorm2d(64),\n",
    "                torch.nn.ReLU())\n",
    "        else:\n",
    "            self.conv5 = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "                torch.nn.ReLU())\n",
    "\n",
    "        \n",
    "        # Sixth conv layer\n",
    "        self.conv6 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        if dropout_6:\n",
    "            self.dropout6 = torch.nn.Dropout(p=dropout_value)\n",
    "        else:\n",
    "          self.dropout6 = torch.nn.Dropout(p=0)\n",
    "\n",
    "        # Linear layers\n",
    "        self.linear1 = torch.nn.Linear(in_features=1024, out_features=n_features1)\n",
    "        self.linear2 = torch.nn.Linear(in_features=n_features1, out_features=n_features2)\n",
    "        self.output = torch.nn.Linear(in_features=n_features2, out_features=200)\n",
    "        self.dropout_l = torch.nn.Dropout(p=dropout_value)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.dropout6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout_l(x)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:21:20.470601Z",
     "iopub.status.busy": "2021-11-23T21:21:20.470330Z",
     "iopub.status.idle": "2021-11-23T21:21:20.475781Z",
     "shell.execute_reply": "2021-11-23T21:21:20.475082Z",
     "shell.execute_reply.started": "2021-11-23T21:21:20.470571Z"
    },
    "id": "c08WAH2UoKzz"
   },
   "outputs": [],
   "source": [
    "def get_model(device, dropout_value, dropout_2, dropout_4, dropout_6, batchnorm_3,\n",
    "              batchnorm_5, n_features1, n_features2):\n",
    "    seed_everything(13)\n",
    "    model = Net(dropout_value, dropout_2, dropout_4, dropout_6, batchnorm_3, batchnorm_5, n_features1, n_features2)\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQYIYd4Ux68r"
   },
   "source": [
    "### Параметры и тд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:21:28.678481Z",
     "iopub.status.busy": "2021-11-23T21:21:28.677624Z",
     "iopub.status.idle": "2021-11-23T21:21:30.993258Z",
     "shell.execute_reply": "2021-11-23T21:21:30.992564Z",
     "shell.execute_reply.started": "2021-11-23T21:21:28.678444Z"
    },
    "id": "MJ9MbWHqyCfB",
    "outputId": "111472a9-4dee-4efd-ca97-0f71d0eb24d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'minimize', 'name': 'mean train loss'},\n",
      " 'parameters': {'batch_size': {'values': [16, 32, 64, 128, 256]},\n",
      "                'batchnorm_3': {'values': [True, False]},\n",
      "                'batchnorm_5': {'values': [True, False]},\n",
      "                'dropout_2': {'values': [True, False]},\n",
      "                'dropout_4': {'values': [True, False]},\n",
      "                'dropout_6': {'values': [True, False]},\n",
      "                'dropout_value': {'values': [0.3, 0.4, 0.5, 0.6]},\n",
      "                'epochs': {'value': 10},\n",
      "                'learning_rate': {'values': [0.0001,\n",
      "                                             0.0005,\n",
      "                                             0.001,\n",
      "                                             0.005,\n",
      "                                             0.01,\n",
      "                                             0.05]},\n",
      "                'n_features1': {'values': [128, 256, 512]},\n",
      "                'n_features2': {'values': [128, 256, 512]},\n",
      "                'optimizer': {'values': ['adam', 'sgd']}}}\n",
      "Create sweep with ID: v8c9b4gw\n",
      "Sweep URL: https://wandb.ai/checheanya/hw2/sweeps/v8c9b4gw\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "metric = {\n",
    "    'name': 'mean train loss',\n",
    "    'goal': 'minimize'   \n",
    "    }\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'n_features1': {\n",
    "        'values': [128, 256, 512]\n",
    "        },\n",
    "    'n_features2': {\n",
    "        'values': [128, 256, 512]\n",
    "        },\n",
    "    'dropout_value': {\n",
    "          'values': [0.3, 0.4, 0.5, 0.6]\n",
    "        },\n",
    "    'dropout_2': {\n",
    "          'values': [True, False]\n",
    "        },\n",
    "    'dropout_4': {\n",
    "          'values': [True, False]\n",
    "        },\n",
    "    'dropout_6': {\n",
    "          'values': [True, False]\n",
    "        },\n",
    "    'batchnorm_3': {\n",
    "          'values': [True, False]\n",
    "        },\n",
    "    'batchnorm_5': {\n",
    "          'values': [True, False]\n",
    "        },\n",
    "    'epochs': {\n",
    "        'value': 10\n",
    "        },\n",
    "    'learning_rate': {\n",
    "        'values': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
    "      },\n",
    "    'batch_size': {\n",
    "        'values': [16, 32, 64, 128, 256],\n",
    "      }\n",
    "    }\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': metric,\n",
    "    'parameters': parameters_dict,\n",
    "    }\n",
    "\n",
    "pprint.pprint(sweep_config)\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"hw2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxR3gfcilRVW"
   },
   "source": [
    "### Обучение модели, запуски экспериментов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CesoOl6BlRVY"
   },
   "source": [
    "Простой тест на проверку правильности написанного кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:34:19.042181Z",
     "iopub.status.busy": "2021-11-23T21:34:19.041712Z",
     "iopub.status.idle": "2021-11-23T21:34:49.838913Z",
     "shell.execute_reply": "2021-11-23T21:34:49.838106Z",
     "shell.execute_reply.started": "2021-11-23T21:34:19.042148Z"
    },
    "id": "B_LB2jn6lRVY",
    "outputId": "9ac2a5d5-afe9-4b0d-8264-56c96d4a7912"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c74434fc2bf49c5a2968f80141ec8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests passed\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model = get_model(device, 0.3, False, False, False, True, True, 256, 128)\n",
    "train_dataloader, test_dataloader = data_loaders(32)\n",
    "\n",
    "metrics, all_losses, predicted_labels, true_labels = predict(model, test_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(predicted_labels, true_labels)\n",
    "print(\"tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tS-LLiXUlRVY"
   },
   "source": [
    "Запустить обучение можно в ячейке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uN94C8I6ELPW",
    "outputId": "119aa92d-3c5d-4ddf-f7d9-2109b1ab5437"
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=10) # this cell was done before, but my notebook decided to restart, so the output is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:44:46.187305Z",
     "iopub.status.busy": "2021-11-23T21:44:46.187044Z",
     "iopub.status.idle": "2021-11-23T21:44:46.191136Z",
     "shell.execute_reply": "2021-11-23T21:44:46.190358Z",
     "shell.execute_reply.started": "2021-11-23T21:44:46.187277Z"
    },
    "id": "ECIzZ_RYlRVZ"
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImVW8_EXlRVZ"
   },
   "source": [
    "### Проверка полученной accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmR-elhJlRVZ"
   },
   "source": [
    "После всех экспериментов которые вы проделали, выберите лучшую из своих моделей, реализуйте и запустите функцию `evaluate`. Эта функция должна брать на вход модель и даталоадер с валидационными данными и возврашать accuracy, посчитанную на этом датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-23T21:58:37.617345Z",
     "iopub.status.busy": "2021-11-23T21:58:37.616836Z",
     "iopub.status.idle": "2021-11-23T21:58:37.636572Z",
     "shell.execute_reply": "2021-11-23T21:58:37.635712Z",
     "shell.execute_reply.started": "2021-11-23T21:58:37.617308Z"
    },
    "id": "3TGH0EFalRVb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5531\n",
      "Оценка за это задание составит 5 баллов\n"
     ]
    }
   ],
   "source": [
    "#metrics, all_losses, predicted_labels, true_labels = predict(model, val_dataloader, criterion, device)\n",
    "assert len(predicted_labels) == len(val_dataset)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(accuracy)\n",
    "print(\"Оценка за это задание составит {} баллов\".format(min(5, 5 * accuracy / 0.44)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pT8vfPSolRVb"
   },
   "source": [
    "### Отчёт об экспериментах \n",
    "\n",
    "Поскольку я использовала для проведения экспериментов wandb, я написала отчет там же, так как только там можно посмотреть графики: https://wandb.ai/checheanya/hw2/reports/Performance-comparison-HW2--VmlldzoxMjU4OTM4?accessToken=4tcfx7cbnpbfhcv86b9kjtccrnu572kb1o5p4iauki5cpgvphgqbwa4fkoacbq3c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
